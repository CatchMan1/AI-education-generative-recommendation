import pandas as pd
import torch
import torch.utils.data as data
import numpy as np
import h5py
import platform
import sys
from transformers import AutoModel, AutoTokenizer
class EmbDataset(data.Dataset):
    '''
    ç›®å‰æ•°æ®é›†æš‚æ— user_profileå­—æ®µ
    è¿™é‡Œé‡‡ç”¨user_nameä»£æ›¿user_profileä½œä¸ºemb.
    æ¨¡å‹çš„è¾“å…¥æ•°æ®æ˜¯[user_profile_emb, item_seq_emb], [target_emb]ï¼Œé€šè¿‡å¼•å…¥ç”¨æˆ·çš„profileå¢åŠ ä¸ªæ€§åŒ–æ¨èçš„æ•ˆæœ
    '''
    def __init__(self, rec_path, course_path, course_id_map):
        self.rec_path = rec_path
        self.course_path = course_path
        self.course_id_map = course_id_map
        self.rec_data, self.item_data, self.course_id_map = self.loading_data()
        self.item_info = self.mapping_id() # {num_id: item_info}
        self.tokenizer, self.model = self.load_plm()
        self.item_embs = self.generate_item_embedding(self.item_info, self.tokenizer, self.model)
        
        # ç”Ÿæˆç”¨æˆ·profileçš„embeddingï¼ˆåŸºäºå§“åï¼‰
        self.user_profile_map = self.extract_user_profiles()
        self.user_profile_embs = self.generate_user_profile_embedding(self.user_profile_map, self.tokenizer, self.model)
        # æ„å»ºåºåˆ—æ¨èçš„è®­ç»ƒæ ·æœ¬
        self.samples, self.sample_user_ids = self.build_sequence_samples()

    def mapping_id(self):
        item_info = {}
        for item in self.item_data:
            for id, num_id in self.course_id_map.items():
                if id == item[0]:
                    item_info[num_id] = item[1] + item[2]
                    break
        return item_info

    def load_plm(self, model_name='bert-base-uncased'):
        
        # 1. è‡ªåŠ¨æ£€æµ‹è®¾å¤‡ (å¦‚æœæœ‰æ˜¾å¡å°±ç”¨ cudaï¼Œå¦åˆ™ç”¨ cpu)
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ğŸš€ æ­£åœ¨åŠ è½½æ¨¡å‹ï¼Œå½“å‰ä½¿ç”¨è®¾å¤‡: {device}")

        # 2. åŠ è½½åˆ†è¯å™¨
        tokenizer = AutoTokenizer.from_pretrained(model_name)

        # 3. åŠ è½½æ ‡å‡†æ¨¡å‹å¹¶ç›´æ¥ç§»åŠ¨åˆ°å¯¹åº”è®¾å¤‡
        model = AutoModel.from_pretrained(model_name).to(device)

        print(f"âœ… æ¨¡å‹å·²æˆåŠŸåŠ è½½è‡³ {device}")
        
        return tokenizer, model

    def generate_item_embedding(self, item_text_dic, tokenizer, model):
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        max_item_id = max(item_text_dic.keys())
        order_texts = ["" if k == 0 else item_text_dic.get(k, "") for k in range(max_item_id + 1)]
        embeddings = []
        start, batch_size = 0, 20
        
        while start < len(order_texts):
            sentences = order_texts[start: start + batch_size]
            encoded_sentences = tokenizer(sentences, padding=True, max_length=512,
                                        truncation=True, return_tensors='pt').to(device)
            outputs = model(**encoded_sentences)
            # è®¡ç®—å¹³å‡æ± åŒ–åµŒå…¥
            masked_output = outputs.last_hidden_state * encoded_sentences['attention_mask'].unsqueeze(-1)
            mean_output = masked_output[:,1:,:].sum(dim=1) / encoded_sentences['attention_mask'][:,1:].sum(dim=-1, keepdim=True)
            mean_output = mean_output.detach()
            embeddings.append(mean_output)
            start += batch_size
        
        embeddings = torch.cat(embeddings, dim=0).cpu().numpy()
        print('Item embeddings shape: ', embeddings.shape)
        return embeddings
    
    def generate_user_profile_embedding(self, user_profile_map, tokenizer, model):
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        user_embs = {}
        sorted_users = sorted(user_profile_map.items(), key=lambda x: x[0])
        batch_size = 32
        
        for start_idx in range(0, len(sorted_users), batch_size):
            batch_users = sorted_users[start_idx:start_idx + batch_size]
            user_ids = [u[0] for u in batch_users]
            user_names = [u[1] for u in batch_users]
            
            encoded = tokenizer(user_names, padding=True, max_length=64,
                              truncation=True, return_tensors='pt').to(device)
            
            with torch.no_grad():
                outputs = model(**encoded)
                # ä½¿ç”¨[CLS] tokençš„embeddingä½œä¸ºç”¨æˆ·è¡¨ç¤º
                user_batch_embs = outputs.last_hidden_state[:, 0, :].cpu().numpy()
            
            for user_id, emb in zip(user_ids, user_batch_embs):
                user_embs[user_id] = emb
        
        print(f"User profile embeddings shape: ({len(user_embs)}, {list(user_embs.values())[0].shape[0]})")
        return user_embs
    
    def loading_data(self):
        rec_data = []
        with h5py.File(self.rec_path, 'r') as f1:
            user_ids = f1['user_id'][:]          # æå–æ‰€æœ‰ user_id
            user_profile = f1['user_profile'].asstr(encoding='utf-8')[:] # æå–æ‰€æœ‰ user_profile
            item_lists = f1['item_id_list'][:]    # æå–æ‰€æœ‰å˜é•¿æ•°ç»„
            rec_data = list(zip(user_ids, user_profile, item_lists))        

        item_data = []
        with h5py.File(self.course_path, 'r') as f2:
            item_ids = f2['item_id'].asstr(encoding='utf-8')[:]        # æå–æ‰€æœ‰ user_id
            item_name = f2['item_name'].asstr(encoding='utf-8')[:]
            item_info = f2['item_info'].asstr(encoding='utf-8')[:]  # æå–æ‰€æœ‰å˜é•¿æ•°ç»„
            item_data = list(zip(item_ids, item_name, item_info))

        course_id_map = {}
        with h5py.File(self.course_id_map, 'r') as f3:
            course_id = f3['item_id'].asstr(encoding='utf-8')[:]
            course_num_id = f3['item_num_id'][:]
            course_id_map = dict(zip(course_id, course_num_id))
        
        return rec_data, item_data, course_id_map

    def build_sequence_samples(self, min_seq_len=2, max_seq_len=20):
        samples = []
        user_ids = []
        
        for user_id, user_name, item_list in self.rec_data:
            item_list = item_list.tolist() if isinstance(item_list, np.ndarray) else item_list
            
            # è¿‡æ»¤æ‰è¿‡çŸ­çš„åºåˆ—
            if len(item_list) < min_seq_len:
                continue
            
            # æ»‘åŠ¨çª—å£ç”Ÿæˆå¤šä¸ªè®­ç»ƒæ ·æœ¬
            for i in range(1, len(item_list)):
                history = item_list[max(0, i-max_seq_len):i]

                target = item_list[i]
                
                samples.append((history, target))
                user_ids.append(user_id)  # ä¿å­˜å¯¹åº”çš„user_id
        
        return samples, user_ids

    def get_sequence_embeddings(self, item_ids):
        embeddings = []
        for item_id in item_ids:
            embeddings.append(self.item_embs[item_id])
        return np.array(embeddings)

    def extract_user_profiles(self):
        user_profile_map = {}
        for user_id, user_name, item_list in self.rec_data:
            user_profile_map[user_id] = user_name
        return user_profile_map
    
    def __getitem__(self, index):
        history_ids, target_id = self.samples[index]
        user_id = self.sample_user_ids[index]
        # è·å–ç”¨æˆ·profile embedding
        user_emb = self.user_profile_embs[user_id]
        # è·å–å†å²åºåˆ—çš„embeddings
        seq_embs = self.get_sequence_embeddings(history_ids)
        
        # å°†user_embæ‹¼æ¥åˆ°åºåˆ—å¼€å¤´ [user_emb, item1, item2, ...]
        user_emb_expanded = np.expand_dims(user_emb, axis=0)  # (1, 768)
        full_seq_embs = np.concatenate([user_emb_expanded, seq_embs], axis=0)  # (seq_len+1, 768)
        # è·å–ç›®æ ‡itemçš„embedding
        target_emb = self.item_embs[target_id]
        # è½¬æ¢ä¸ºtensor
        seq_embs = torch.FloatTensor(full_seq_embs)
        target_emb = torch.FloatTensor(target_emb)
        
        return {
            'seq_embs': seq_embs,   # (seq_len+1, 768) ç¬¬0ä½æ˜¯user_embï¼Œåé¢æ˜¯itemåºåˆ—
            'target_emb': target_emb,    # (768,)
            'seq_len': len(history_ids) + 1,  # åŒ…å«user_embçš„æ€»é•¿åº¦
            'target_id': target_id,      # ç›®æ ‡itemçš„IDï¼ˆç”¨äºè¯„ä¼°ï¼‰
            'user_id': user_id           # ç”¨æˆ·ID
        }

    def __len__(self):
        return len(self.samples)